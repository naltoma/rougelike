#!/usr/bin/env python3
"""
å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã¨å“è³ªä¿è¨¼ã‚·ã‚¹ãƒ†ãƒ 

å­¦ç¿’åŠ¹æœã®æ¸¬å®šã€ã‚³ãƒ¼ãƒ‰å“è³ªã®è©•ä¾¡ã€ã‚·ã‚¹ãƒ†ãƒ ã®ä¿¡é ¼æ€§ç¢ºä¿ã‚’è¡Œã†
"""

import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, field
from pathlib import Path
from enum import Enum
import statistics
import re


class CodeQuality(Enum):
    """ã‚³ãƒ¼ãƒ‰å“è³ªãƒ¬ãƒ™ãƒ«"""
    POOR = "poor"           # æ”¹å–„ãŒå¿…è¦
    FAIR = "fair"           # ã¾ãšã¾ãš
    GOOD = "good"           # è‰¯ã„
    EXCELLENT = "excellent" # å„ªç§€


class LearningEfficiency(Enum):
    """å­¦ç¿’åŠ¹ç‡ãƒ¬ãƒ™ãƒ«"""
    STRUGGLING = "struggling"   # è‹¦æˆ¦ä¸­
    LEARNING = "learning"       # å­¦ç¿’ä¸­
    PROGRESSING = "progressing" # é †èª¿
    MASTERING = "mastering"     # ç¿’å¾—ä¸­


@dataclass
class CodeMetrics:
    """ã‚³ãƒ¼ãƒ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    lines_of_code: int = 0
    complexity_score: float = 0.0
    readability_score: float = 0.0
    efficiency_score: float = 0.0
    error_density: float = 0.0
    
    # è©³ç´°ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    cyclomatic_complexity: int = 0
    duplicate_code_ratio: float = 0.0
    api_usage_diversity: int = 0
    comments_ratio: float = 0.0
    
    @property
    def overall_quality(self) -> CodeQuality:
        """å…¨ä½“çš„ãªã‚³ãƒ¼ãƒ‰å“è³ªã‚’è©•ä¾¡"""
        scores = [
            self.readability_score,
            self.efficiency_score,
            1.0 - self.error_density,  # ã‚¨ãƒ©ãƒ¼å¯†åº¦ã¯ä½ã„ã»ã©è‰¯ã„
            min(1.0, self.api_usage_diversity / 5.0)  # APIä½¿ç”¨å¤šæ§˜æ€§
        ]
        
        avg_score = sum(scores) / len(scores)
        
        if avg_score >= 0.9:
            return CodeQuality.EXCELLENT
        elif avg_score >= 0.7:
            return CodeQuality.GOOD
        elif avg_score >= 0.5:
            return CodeQuality.FAIR
        else:
            return CodeQuality.POOR


@dataclass
class LearningMetrics:
    """å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    session_duration: timedelta = field(default_factory=lambda: timedelta())
    total_attempts: int = 0
    successful_attempts: int = 0
    error_count: int = 0
    hint_usage_count: int = 0
    
    # å­¦ç¿’è¡Œå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    average_response_time: float = 0.0
    consistency_score: float = 0.0
    improvement_rate: float = 0.0
    exploration_score: float = 0.0
    
    # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
    attempt_timestamps: List[datetime] = field(default_factory=list)
    success_pattern: List[bool] = field(default_factory=list)
    response_times: List[float] = field(default_factory=list)
    
    @property
    def success_rate(self) -> float:
        """æˆåŠŸç‡"""
        if self.total_attempts == 0:
            return 0.0
        return self.successful_attempts / self.total_attempts
    
    @property
    def error_rate(self) -> float:
        """ã‚¨ãƒ©ãƒ¼ç‡"""
        if self.total_attempts == 0:
            return 0.0
        return self.error_count / self.total_attempts
    
    @property
    def learning_efficiency(self) -> LearningEfficiency:
        """å­¦ç¿’åŠ¹ç‡ã‚’è©•ä¾¡"""
        success_rate = self.success_rate
        improvement = self.improvement_rate
        consistency = self.consistency_score
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        efficiency_score = (success_rate * 0.4 + 
                           improvement * 0.3 + 
                           consistency * 0.3)
        
        if efficiency_score >= 0.8:
            return LearningEfficiency.MASTERING
        elif efficiency_score >= 0.6:
            return LearningEfficiency.PROGRESSING
        elif efficiency_score >= 0.4:
            return LearningEfficiency.LEARNING
        else:
            return LearningEfficiency.STRUGGLING
    
    def add_attempt(self, success: bool, response_time: float = 0.0) -> None:
        """è©¦è¡Œè¨˜éŒ²ã‚’è¿½åŠ """
        self.total_attempts += 1
        if success:
            self.successful_attempts += 1
        
        self.attempt_timestamps.append(datetime.now())
        self.success_pattern.append(success)
        if response_time > 0:
            self.response_times.append(response_time)
        
        self._update_derived_metrics()
    
    def _update_derived_metrics(self) -> None:
        """æ´¾ç”Ÿãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°"""
        if self.response_times:
            self.average_response_time = statistics.mean(self.response_times)
        
        # æ”¹å–„ç‡è¨ˆç®—ï¼ˆæœ€è¿‘ã®æˆåŠŸç‡ã¨åˆæœŸã®æˆåŠŸç‡ã‚’æ¯”è¼ƒï¼‰
        if len(self.success_pattern) >= 10:
            recent_success = sum(self.success_pattern[-5:]) / 5
            initial_success = sum(self.success_pattern[:5]) / 5
            self.improvement_rate = max(0.0, recent_success - initial_success)
        
        # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ï¼ˆæˆåŠŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®‰å®šæ€§ï¼‰
        if len(self.success_pattern) >= 5:
            recent_patterns = self.success_pattern[-10:]
            if recent_patterns:
                variance = statistics.variance([1 if x else 0 for x in recent_patterns])
                self.consistency_score = max(0.0, 1.0 - variance)


@dataclass
class QualityReport:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆ"""
    student_id: str
    session_id: str
    timestamp: datetime
    code_metrics: CodeMetrics
    learning_metrics: LearningMetrics
    
    # çµ±åˆè©•ä¾¡
    overall_score: float = 0.0
    recommendations: List[str] = field(default_factory=list)
    achievements: List[str] = field(default_factory=list)
    
    def generate_summary(self) -> str:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        return f"""
ğŸ“Š å­¦ç¿’å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
==================
å­¦ç”ŸID: {self.student_id}
ã‚»ãƒƒã‚·ãƒ§ãƒ³: {self.session_id}
è©•ä¾¡æ™‚åˆ»: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“ˆ å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹:
  æˆåŠŸç‡: {self.learning_metrics.success_rate:.1%}
  å­¦ç¿’åŠ¹ç‡: {self.learning_metrics.learning_efficiency.value}
  æ”¹å–„ç‡: {self.learning_metrics.improvement_rate:.1%}
  å¹³å‡å¿œç­”æ™‚é–“: {self.learning_metrics.average_response_time:.1f}ç§’

ğŸ’» ã‚³ãƒ¼ãƒ‰å“è³ª:
  å…¨ä½“å“è³ª: {self.code_metrics.overall_quality.value}
  å¯èª­æ€§: {self.code_metrics.readability_score:.1%}
  åŠ¹ç‡æ€§: {self.code_metrics.efficiency_score:.1%}
  ã‚¨ãƒ©ãƒ¼å¯†åº¦: {self.code_metrics.error_density:.3f}

ğŸ† ç·åˆã‚¹ã‚³ã‚¢: {self.overall_score:.1%}

ğŸ’¡ æ¨å¥¨äº‹é …:
{chr(10).join(f"  â€¢ {rec}" for rec in self.recommendations)}

ğŸ‰ é”æˆé …ç›®:
{chr(10).join(f"  â€¢ {ach}" for ach in self.achievements)}
"""


class CodeAnalyzer:
    """ã‚³ãƒ¼ãƒ‰åˆ†æå™¨"""
    
    def __init__(self):
        self.api_patterns = {
            "move", "turn_left", "turn_right", "attack", "pickup", "see",
            "can_undo", "undo", "is_game_finished", "get_game_result"
        }
    
    def analyze_code_quality(self, code_text: str, api_calls: List[str]) -> CodeMetrics:
        """ã‚³ãƒ¼ãƒ‰å“è³ªã‚’åˆ†æ"""
        metrics = CodeMetrics()
        
        if not code_text:
            return metrics
        
        lines = code_text.split('\n')
        metrics.lines_of_code = len([line for line in lines if line.strip()])
        
        # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
        metrics.cyclomatic_complexity = self._calculate_complexity(code_text)
        metrics.readability_score = self._calculate_readability(code_text)
        metrics.efficiency_score = self._calculate_efficiency(code_text, api_calls)
        metrics.api_usage_diversity = len(set(api_calls))
        metrics.comments_ratio = self._calculate_comments_ratio(code_text)
        metrics.duplicate_code_ratio = self._calculate_duplication(code_text)
        
        return metrics
    
    def _calculate_complexity(self, code: str) -> int:
        """å¾ªç’°çš„è¤‡é›‘åº¦ã‚’è¨ˆç®—"""
        complexity = 1  # åŸºæœ¬ãƒ‘ã‚¹
        
        # åˆ¶å¾¡æ§‹é€ ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        patterns = [
            r'\bif\b', r'\belif\b', r'\belse\b',
            r'\bwhile\b', r'\bfor\b',
            r'\btry\b', r'\bexcept\b',
            r'\band\b', r'\borg\b'
        ]
        
        for pattern in patterns:
            complexity += len(re.findall(pattern, code))
        
        return complexity
    
    def _calculate_readability(self, code: str) -> float:
        """ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ã‚’è¨ˆç®—"""
        if not code.strip():
            return 0.0
        
        lines = [line.strip() for line in code.split('\n') if line.strip()]
        if not lines:
            return 0.0
        
        # å¯èª­æ€§è¦ç´ ã‚’è©•ä¾¡
        score_factors = []
        
        # é©åˆ‡ãªé•·ã•ã®è¡Œï¼ˆçŸ­ã™ããšé•·ã™ããªã„ï¼‰
        good_length_lines = sum(1 for line in lines if 10 <= len(line) <= 80)
        score_factors.append(good_length_lines / len(lines))
        
        # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã®ä¸€è²«æ€§
        indents = [len(line) - len(line.lstrip()) for line in lines if line.strip()]
        if indents:
            indent_consistency = 1.0 - (statistics.stdev(indents) / 10.0 if len(indents) > 1 else 0.0)
            score_factors.append(max(0.0, min(1.0, indent_consistency)))
        
        # ç©ºè¡Œã®ä½¿ç”¨ï¼ˆé©åˆ‡ãªåˆ†å‰²ï¼‰
        empty_lines = len([line for line in code.split('\n') if not line.strip()])
        empty_ratio = empty_lines / len(code.split('\n'))
        score_factors.append(min(1.0, empty_ratio * 5))  # é©åº¦ãªç©ºè¡Œã¯è‰¯ã„
        
        return sum(score_factors) / len(score_factors) if score_factors else 0.0
    
    def _calculate_efficiency(self, code: str, api_calls: List[str]) -> float:
        """åŠ¹ç‡æ€§ã‚’è¨ˆç®—"""
        if not api_calls:
            return 0.0
        
        efficiency_factors = []
        
        # APIå‘¼ã³å‡ºã—ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        unique_calls = set(api_calls)
        if len(api_calls) > 0:
            diversity_ratio = len(unique_calls) / len(api_calls)
            efficiency_factors.append(diversity_ratio)
        
        # see()ã®é©åˆ‡ãªä½¿ç”¨
        see_calls = api_calls.count('see')
        action_calls = len([call for call in api_calls 
                           if call in ['move', 'attack', 'pickup']])
        
        if action_calls > 0:
            see_ratio = see_calls / action_calls
            # é©åº¦ãªsee()ä½¿ç”¨ãŒæœ›ã¾ã—ã„ï¼ˆ0.2-0.5ãŒç†æƒ³ï¼‰
            optimal_see_score = 1.0 - abs(see_ratio - 0.35) / 0.35
            efficiency_factors.append(max(0.0, optimal_see_score))
        
        # ãƒ«ãƒ¼ãƒ—ã®æ¤œå‡ºã¨è©•ä¾¡
        loop_patterns = len(re.findall(r'\b(for|while)\b', code))
        if loop_patterns > 0:
            efficiency_factors.append(0.8)  # ãƒ«ãƒ¼ãƒ—ä½¿ç”¨ã¯åŠ¹ç‡çš„
        
        return sum(efficiency_factors) / len(efficiency_factors) if efficiency_factors else 0.0
    
    def _calculate_comments_ratio(self, code: str) -> float:
        """ã‚³ãƒ¡ãƒ³ãƒˆç‡ã‚’è¨ˆç®—"""
        lines = code.split('\n')
        comment_lines = sum(1 for line in lines if line.strip().startswith('#'))
        code_lines = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
        
        if code_lines == 0:
            return 0.0
        
        return comment_lines / (code_lines + comment_lines)
    
    def _calculate_duplication(self, code: str) -> float:
        """é‡è¤‡ã‚³ãƒ¼ãƒ‰ç‡ã‚’è¨ˆç®—"""
        lines = [line.strip() for line in code.split('\n') if line.strip()]
        if len(lines) < 2:
            return 0.0
        
        duplicates = 0
        for i, line in enumerate(lines):
            for j, other_line in enumerate(lines[i+1:], i+1):
                if line == other_line and len(line) > 5:  # çŸ­ã„ãƒ©ã‚¤ãƒ³ä»¥å¤–
                    duplicates += 1
        
        return duplicates / len(lines) if lines else 0.0


class LearningAnalyzer:
    """å­¦ç¿’åˆ†æå™¨"""
    
    def analyze_learning_pattern(self, session_data: List[Dict[str, Any]]) -> LearningMetrics:
        """å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ"""
        metrics = LearningMetrics()
        
        if not session_data:
            return metrics
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ç¶™ç¶šæ™‚é–“
        if len(session_data) > 1:
            start_time = datetime.fromisoformat(session_data[0]['timestamp'])
            end_time = datetime.fromisoformat(session_data[-1]['timestamp'])
            metrics.session_duration = end_time - start_time
        
        # å„ã‚¤ãƒ™ãƒ³ãƒˆã‚’å‡¦ç†
        response_times = []
        last_timestamp = None
        
        for event in session_data:
            event_time = datetime.fromisoformat(event['timestamp'])
            event_type = event.get('event_type', '')
            
            if event_type == 'action_executed':
                success = event.get('data', {}).get('success', False)
                
                # å¿œç­”æ™‚é–“è¨ˆç®—
                if last_timestamp:
                    response_time = (event_time - last_timestamp).total_seconds()
                    response_times.append(response_time)
                    metrics.add_attempt(success, response_time)
                else:
                    metrics.add_attempt(success)
            
            elif event_type == 'error_occurred':
                metrics.error_count += 1
            
            elif event_type == 'hint_used':
                metrics.hint_usage_count += 1
            
            last_timestamp = event_time
        
        # æ¢ç´¢ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç•°ãªã‚‹APIã®ä½¿ç”¨åº¦åˆã„ï¼‰
        api_variety = set()
        for event in session_data:
            if event.get('event_type') == 'action_executed':
                api = event.get('data', {}).get('action', '')
                if api:
                    api_variety.add(api)
        
        metrics.exploration_score = min(1.0, len(api_variety) / 6.0)  # 6ç¨®é¡ã®APIã‚’æƒ³å®š
        
        return metrics


class QualityAssuranceManager:
    """å“è³ªä¿è¨¼ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
    
    def __init__(self, report_dir: str = "data/quality_reports"):
        self.report_dir = Path(report_dir)
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        self.code_analyzer = CodeAnalyzer()
        self.learning_analyzer = LearningAnalyzer()
        
        # å“è³ªåŸºæº–
        self.quality_thresholds = {
            "min_success_rate": 0.3,
            "min_code_quality": CodeQuality.FAIR,
            "max_error_density": 0.5,
            "min_improvement_rate": 0.1
        }
    
    def generate_quality_report(self, student_id: str, session_id: str, 
                              code_text: str, api_calls: List[str],
                              session_data: List[Dict[str, Any]]) -> QualityReport:
        """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        
        # ã‚³ãƒ¼ãƒ‰åˆ†æ
        code_metrics = self.code_analyzer.analyze_code_quality(code_text, api_calls)
        
        # å­¦ç¿’åˆ†æ
        learning_metrics = self.learning_analyzer.analyze_learning_pattern(session_data)
        
        # ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ
        report = QualityReport(
            student_id=student_id,
            session_id=session_id,
            timestamp=datetime.now(),
            code_metrics=code_metrics,
            learning_metrics=learning_metrics
        )
        
        # ç·åˆã‚¹ã‚³ã‚¢è¨ˆç®—
        report.overall_score = self._calculate_overall_score(code_metrics, learning_metrics)
        
        # æ¨å¥¨äº‹é …ç”Ÿæˆ
        report.recommendations = self._generate_recommendations(code_metrics, learning_metrics)
        
        # é”æˆé …ç›®ç”Ÿæˆ
        report.achievements = self._generate_achievements(code_metrics, learning_metrics)
        
        return report
    
    def _calculate_overall_score(self, code_metrics: CodeMetrics, 
                               learning_metrics: LearningMetrics) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        # é‡ã¿ä»˜ãå¹³å‡ã§ã‚¹ã‚³ã‚¢è¨ˆç®—
        weights = {
            'success_rate': 0.25,
            'code_quality': 0.25,
            'learning_efficiency': 0.20,
            'improvement': 0.15,
            'consistency': 0.15
        }
        
        # å„è¦ç´ ã®ã‚¹ã‚³ã‚¢ï¼ˆ0-1ï¼‰
        success_score = learning_metrics.success_rate
        
        code_quality_scores = {
            CodeQuality.POOR: 0.25,
            CodeQuality.FAIR: 0.5,
            CodeQuality.GOOD: 0.75,
            CodeQuality.EXCELLENT: 1.0
        }
        code_score = code_quality_scores.get(code_metrics.overall_quality, 0.5)
        
        efficiency_scores = {
            LearningEfficiency.STRUGGLING: 0.25,
            LearningEfficiency.LEARNING: 0.5,
            LearningEfficiency.PROGRESSING: 0.75,
            LearningEfficiency.MASTERING: 1.0
        }
        efficiency_score = efficiency_scores.get(learning_metrics.learning_efficiency, 0.5)
        
        improvement_score = min(1.0, learning_metrics.improvement_rate * 2)
        consistency_score = learning_metrics.consistency_score
        
        overall_score = (
            weights['success_rate'] * success_score +
            weights['code_quality'] * code_score +
            weights['learning_efficiency'] * efficiency_score +
            weights['improvement'] * improvement_score +
            weights['consistency'] * consistency_score
        )
        
        return overall_score
    
    def _generate_recommendations(self, code_metrics: CodeMetrics, 
                                learning_metrics: LearningMetrics) -> List[str]:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # ã‚³ãƒ¼ãƒ‰å“è³ªã«åŸºã¥ãæ¨å¥¨
        if code_metrics.overall_quality == CodeQuality.POOR:
            recommendations.append("ã‚³ãƒ¼ãƒ‰ã®æ§‹é€ ã‚’è¦‹ç›´ã—ã€ã‚ˆã‚Šèª­ã¿ã‚„ã™ã„æ›¸ãæ–¹ã‚’å¿ƒãŒã‘ã¾ã—ã‚‡ã†")
        
        if code_metrics.readability_score < 0.5:
            recommendations.append("é©åˆ‡ãªã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä½¿ã£ã¦ã€ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿ã‚„ã™ãã—ã¾ã—ã‚‡ã†")
        
        if code_metrics.api_usage_diversity < 3:
            recommendations.append("æ§˜ã€…ãªAPIã‚’è©¦ã—ã¦ã€æ©Ÿèƒ½ã®å¹…ã‚’åºƒã’ã¾ã—ã‚‡ã†")
        
        if code_metrics.efficiency_score < 0.5:
            recommendations.append("see()ã§çŠ¶æ³ç¢ºèªã—ã¦ã‹ã‚‰è¡Œå‹•ã™ã‚‹ç¿’æ…£ã‚’èº«ã«ã¤ã‘ã¾ã—ã‚‡ã†")
        
        # å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«åŸºã¥ãæ¨å¥¨
        if learning_metrics.success_rate < 0.3:
            recommendations.append("åŸºæœ¬çš„ãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å¾©ç¿’ã—ã€æ®µéšçš„ã«å–ã‚Šçµ„ã¿ã¾ã—ã‚‡ã†")
        
        if learning_metrics.error_rate > 0.5:
            recommendations.append("ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ³¨æ„æ·±ãèª­ã¿ã€åŸå› ã‚’ç†è§£ã—ã¾ã—ã‚‡ã†")
        
        if learning_metrics.improvement_rate < 0.1:
            recommendations.append("æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’è©¦ã—ã€å­¦ç¿’æ–¹æ³•ã‚’å¤‰ãˆã¦ã¿ã¾ã—ã‚‡ã†")
        
        if learning_metrics.consistency_score < 0.5:
            recommendations.append("å®‰å®šã—ãŸè§£æ³•ã‚’èº«ã«ã¤ã‘ã‚‹ã¾ã§ã€åŸºæœ¬ã‚’ç¹°ã‚Šè¿”ã—ç·´ç¿’ã—ã¾ã—ã‚‡ã†")
        
        # æ¢ç´¢ã‚¹ã‚³ã‚¢ã«åŸºã¥ãæ¨å¥¨
        if learning_metrics.exploration_score < 0.3:
            recommendations.append("ä»–ã®æ©Ÿèƒ½ã‚‚ç©æ¥µçš„ã«è©¦ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ã‚’æ¢ç´¢ã—ã¦ã¿ã¾ã—ã‚‡ã†")
        
        return recommendations
    
    def _generate_achievements(self, code_metrics: CodeMetrics,
                             learning_metrics: LearningMetrics) -> List[str]:
        """é”æˆé …ç›®ã‚’ç”Ÿæˆ"""
        achievements = []
        
        # å­¦ç¿’ãƒ¡ãƒˆãƒªã‚¯ã‚¹é”æˆ
        if learning_metrics.success_rate >= 0.8:
            achievements.append("é«˜ã„æˆåŠŸç‡ã‚’é”æˆã—ã¾ã—ãŸï¼")
        
        if learning_metrics.improvement_rate >= 0.3:
            achievements.append("é¡•è‘—ãªæ”¹å–„ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼")
        
        if learning_metrics.consistency_score >= 0.8:
            achievements.append("å®‰å®šã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¦ã„ã¾ã™ï¼")
        
        if learning_metrics.learning_efficiency == LearningEfficiency.MASTERING:
            achievements.append("ç¿’å¾—æ®µéšã«åˆ°é”ã—ã¾ã—ãŸï¼")
        
        # ã‚³ãƒ¼ãƒ‰å“è³ªé”æˆ
        if code_metrics.overall_quality == CodeQuality.EXCELLENT:
            achievements.append("å„ªç§€ãªã‚³ãƒ¼ãƒ‰å“è³ªã‚’é”æˆã—ã¾ã—ãŸï¼")
        
        if code_metrics.readability_score >= 0.8:
            achievements.append("èª­ã¿ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã‘ã¦ã„ã¾ã™ï¼")
        
        if code_metrics.efficiency_score >= 0.8:
            achievements.append("åŠ¹ç‡çš„ãªãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ä½œæˆã—ã¾ã—ãŸï¼")
        
        if code_metrics.api_usage_diversity >= 5:
            achievements.append("æ§˜ã€…ãªæ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã¦ã„ã¾ã™ï¼")
        
        # ç‰¹åˆ¥ãªé”æˆ
        if learning_metrics.error_rate < 0.1:
            achievements.append("ã‚¨ãƒ©ãƒ¼ã®å°‘ãªã„å„ªç§€ãªå®Ÿè£…ã§ã™ï¼")
        
        if learning_metrics.exploration_score >= 0.8:
            achievements.append("ç©æ¥µçš„ãªæ¢ç´¢å§¿å‹¢ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼")
        
        return achievements
    
    def save_report(self, report: QualityReport) -> Path:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        filename = f"{report.student_id}_{report.session_id}_{report.timestamp.strftime('%Y%m%d_%H%M%S')}.json"
        filepath = self.report_dir / filename
        
        # ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
        report_data = {
            "student_id": report.student_id,
            "session_id": report.session_id,
            "timestamp": report.timestamp.isoformat(),
            "overall_score": report.overall_score,
            "code_metrics": {
                "lines_of_code": report.code_metrics.lines_of_code,
                "complexity_score": report.code_metrics.complexity_score,
                "readability_score": report.code_metrics.readability_score,
                "efficiency_score": report.code_metrics.efficiency_score,
                "error_density": report.code_metrics.error_density,
                "overall_quality": report.code_metrics.overall_quality.value
            },
            "learning_metrics": {
                "session_duration_seconds": report.learning_metrics.session_duration.total_seconds(),
                "total_attempts": report.learning_metrics.total_attempts,
                "successful_attempts": report.learning_metrics.successful_attempts,
                "success_rate": report.learning_metrics.success_rate,
                "error_count": report.learning_metrics.error_count,
                "learning_efficiency": report.learning_metrics.learning_efficiency.value,
                "improvement_rate": report.learning_metrics.improvement_rate,
                "consistency_score": report.learning_metrics.consistency_score
            },
            "recommendations": report.recommendations,
            "achievements": report.achievements
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        return filepath
    
    def load_report(self, filepath: Path) -> Optional[QualityReport]:
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # ãƒ‡ãƒ¼ã‚¿å¾©å…ƒï¼ˆç°¡ç•¥ç‰ˆï¼‰
            code_metrics = CodeMetrics(
                lines_of_code=data["code_metrics"]["lines_of_code"],
                readability_score=data["code_metrics"]["readability_score"],
                efficiency_score=data["code_metrics"]["efficiency_score"],
                error_density=data["code_metrics"]["error_density"]
            )
            
            learning_metrics = LearningMetrics(
                session_duration=timedelta(seconds=data["learning_metrics"]["session_duration_seconds"]),
                total_attempts=data["learning_metrics"]["total_attempts"],
                successful_attempts=data["learning_metrics"]["successful_attempts"],
                error_count=data["learning_metrics"]["error_count"]
            )
            
            report = QualityReport(
                student_id=data["student_id"],
                session_id=data["session_id"],
                timestamp=datetime.fromisoformat(data["timestamp"]),
                code_metrics=code_metrics,
                learning_metrics=learning_metrics,
                overall_score=data["overall_score"],
                recommendations=data["recommendations"],
                achievements=data["achievements"]
            )
            
            return report
            
        except Exception as e:
            print(f"ãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_student_reports(self, student_id: str) -> List[QualityReport]:
        """å­¦ç”Ÿã®å…¨ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
        reports = []
        pattern = f"{student_id}_*.json"
        
        for filepath in self.report_dir.glob(pattern):
            report = self.load_report(filepath)
            if report:
                reports.append(report)
        
        # æ™‚åˆ»é †ã«ã‚½ãƒ¼ãƒˆ
        reports.sort(key=lambda r: r.timestamp)
        return reports
    
    def generate_progress_summary(self, student_id: str) -> Dict[str, Any]:
        """é€²æ—ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ"""
        reports = self.get_student_reports(student_id)
        
        if not reports:
            return {"error": "ãƒ¬ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # çµ±è¨ˆè¨ˆç®—
        scores = [r.overall_score for r in reports]
        success_rates = [r.learning_metrics.success_rate for r in reports]
        
        summary = {
            "student_id": student_id,
            "total_sessions": len(reports),
            "date_range": {
                "start": reports[0].timestamp.isoformat(),
                "end": reports[-1].timestamp.isoformat()
            },
            "overall_progress": {
                "average_score": statistics.mean(scores),
                "latest_score": scores[-1],
                "score_trend": scores[-1] - scores[0] if len(scores) > 1 else 0,
                "highest_score": max(scores),
                "consistency": 1.0 - statistics.stdev(scores) if len(scores) > 1 else 1.0
            },
            "learning_statistics": {
                "average_success_rate": statistics.mean(success_rates),
                "latest_success_rate": success_rates[-1],
                "improvement_trend": success_rates[-1] - success_rates[0] if len(success_rates) > 1 else 0
            },
            "achievements_summary": {
                "total_achievements": sum(len(r.achievements) for r in reports),
                "unique_achievements": len(set().union(*[r.achievements for r in reports])),
                "recent_achievements": reports[-1].achievements if reports else []
            }
        }
        
        return summary


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_quality_manager = QualityAssuranceManager()


def generate_quality_report(student_id: str, session_id: str, code_text: str, 
                          api_calls: List[str], session_data: List[Dict[str, Any]]) -> QualityReport:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ï¼‰"""
    return _quality_manager.generate_quality_report(student_id, session_id, code_text, api_calls, session_data)


def save_quality_report(report: QualityReport) -> str:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ï¼‰"""
    filepath = _quality_manager.save_report(report)
    return str(filepath)


def get_student_progress_summary(student_id: str) -> Dict[str, Any]:
    """å­¦ç”Ÿã®é€²æ—ã‚µãƒãƒªãƒ¼ã‚’å–å¾—ï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ï¼‰"""
    return _quality_manager.generate_progress_summary(student_id)


def analyze_code_quality(code_text: str, api_calls: List[str]) -> Dict[str, Any]:
    """ã‚³ãƒ¼ãƒ‰å“è³ªã‚’åˆ†æï¼ˆã‚°ãƒ­ãƒ¼ãƒãƒ«é–¢æ•°ï¼‰"""
    analyzer = CodeAnalyzer()
    metrics = analyzer.analyze_code_quality(code_text, api_calls)
    
    return {
        "overall_quality": metrics.overall_quality.value,
        "readability_score": metrics.readability_score,
        "efficiency_score": metrics.efficiency_score,
        "api_usage_diversity": metrics.api_usage_diversity,
        "lines_of_code": metrics.lines_of_code,
        "error_density": metrics.error_density
    }


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆç”¨
__all__ = [
    "CodeQuality", "LearningEfficiency", "CodeMetrics", "LearningMetrics",
    "QualityReport", "CodeAnalyzer", "LearningAnalyzer", "QualityAssuranceManager",
    "generate_quality_report", "save_quality_report", "get_student_progress_summary",
    "analyze_code_quality"
]