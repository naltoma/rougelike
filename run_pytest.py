#!/usr/bin/env python3
"""
pytestçµ±åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å¤±æ•—ãƒ†ã‚¹ãƒˆã®å†å®Ÿè¡Œæ©Ÿèƒ½ã¨è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆä»˜ã
"""

import sys
import os
import subprocess
import json
import time
from pathlib import Path
from typing import List, Dict, Any

def install_pytest_if_needed():
    """pytestãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    try:
        import pytest
        return True
    except ImportError:
        print("ğŸ“¦ pytestãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’è©¦ã¿ã¾ã™...")
        print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("   pip install -r requirements.txt")
        return False

def run_pytest_with_json_report(args: List[str] = None) -> Dict[str, Any]:
    """pytestå®Ÿè¡Œã¨JSONãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    if args is None:
        args = []
    
    # JSONå‡ºåŠ›ç”¨ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«
    json_report_file = "pytest_report.json"
    
    pytest_args = [
        sys.executable, "-m", "pytest",
        "--json-report",
        f"--json-report-file={json_report_file}",
        "-v",
        "--tb=short",
        "--color=yes"
    ] + args
    
    print(f"ğŸ§ª å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(pytest_args)}")
    print("=" * 80)
    
    start_time = time.time()
    result = subprocess.run(pytest_args, capture_output=False, text=True)
    end_time = time.time()
    
    execution_time = end_time - start_time
    
    # JSONãƒ¬ãƒãƒ¼ãƒˆèª­ã¿è¾¼ã¿
    report_data = {}
    if os.path.exists(json_report_file):
        try:
            with open(json_report_file, 'r', encoding='utf-8') as f:
                report_data = json.load(f)
        except json.JSONDecodeError:
            print("âš ï¸ JSONãƒ¬ãƒãƒ¼ãƒˆã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ")
    
    return {
        'returncode': result.returncode,
        'execution_time': execution_time,
        'report': report_data
    }

def analyze_test_results(report_data: Dict[str, Any]) -> Dict[str, Any]:
    """ãƒ†ã‚¹ãƒˆçµæœã‚’åˆ†æ"""
    if not report_data:
        return {}
    
    summary = report_data.get('summary', {})
    tests = report_data.get('tests', [])
    
    passed_tests = [t for t in tests if t['outcome'] == 'passed']
    failed_tests = [t for t in tests if t['outcome'] == 'failed']
    skipped_tests = [t for t in tests if t['outcome'] == 'skipped']
    error_tests = [t for t in tests if t['outcome'] == 'error']
    
    return {
        'total': summary.get('total', 0),
        'passed': len(passed_tests),
        'failed': len(failed_tests),
        'skipped': len(skipped_tests),
        'errors': len(error_tests),
        'passed_tests': passed_tests,
        'failed_tests': failed_tests,
        'skipped_tests': skipped_tests,
        'error_tests': error_tests
    }

def print_detailed_results(analysis: Dict[str, Any], execution_time: float):
    """è©³ç´°ãªçµæœè¡¨ç¤º"""
    if not analysis:
        return
    
    print("\n" + "=" * 80)
    print("ğŸ“Š ãƒ†ã‚¹ãƒˆå®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼")
    print("=" * 80)
    
    total = analysis['total']
    passed = analysis['passed']
    failed = analysis['failed']
    skipped = analysis['skipped']
    errors = analysis['errors']
    
    # æˆåŠŸç‡è¨ˆç®—
    if total > 0:
        success_rate = (passed / total) * 100
    else:
        success_rate = 0
    
    print(f"ğŸ¯ ç·åˆçµæœ: {passed}/{total} ãƒ†ã‚¹ãƒˆæˆåŠŸ")
    print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {execution_time:.2f}ç§’")
    print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
    print(f"âœ… æˆåŠŸ: {passed}")
    print(f"âŒ å¤±æ•—: {failed}")
    print(f"âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {skipped}")
    print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: {errors}")
    
    # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°
    if analysis['failed_tests']:
        print(f"\nâŒ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆ ({len(analysis['failed_tests'])}å€‹):")
        failed_files = []
        for test in analysis['failed_tests']:
            test_name = test.get('nodeid', 'Unknown')
            test_file = test_name.split('::')[0] if '::' in test_name else test_name
            failed_files.append(test_file)
            print(f"   â€¢ {test_name}")
            
            # ã‚¨ãƒ©ãƒ¼å†…å®¹ã‚’ç°¡æ½”ã«è¡¨ç¤º
            if 'call' in test and 'longrepr' in test['call']:
                longrepr = test['call']['longrepr']
                if isinstance(longrepr, str):
                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æœ€å¾Œã®è¡Œã‚’å–å¾—
                    error_lines = longrepr.strip().split('\n')
                    if error_lines:
                        last_line = error_lines[-1].strip()
                        if last_line:
                            print(f"     â””â”€ {last_line}")
        
        # å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®å†å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
        print(f"\nğŸ”„ å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®ã¿å†å®Ÿè¡Œ:")
        unique_failed_files = list(set(failed_files))
        if len(unique_failed_files) <= 5:  # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå°‘ãªã„å ´åˆã¯å€‹åˆ¥ã«è¡¨ç¤º
            for file in unique_failed_files:
                print(f"   pytest {file} -v")
        else:  # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ãŒå¤šã„å ´åˆã¯ä¸€æ‹¬ã‚³ãƒãƒ³ãƒ‰
            print(f"   pytest --lf -v  # å‰å›å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®ã¿")
    
    # ã‚¨ãƒ©ãƒ¼ãƒ†ã‚¹ãƒˆã®è©³ç´°
    if analysis['error_tests']:
        print(f"\nğŸš¨ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãƒ†ã‚¹ãƒˆ ({len(analysis['error_tests'])}å€‹):")
        for test in analysis['error_tests']:
            test_name = test.get('nodeid', 'Unknown')
            print(f"   â€¢ {test_name}")
    
    # ã‚¹ã‚­ãƒƒãƒ—ã—ãŸãƒ†ã‚¹ãƒˆã®è©³ç´°
    if analysis['skipped_tests']:
        print(f"\nâš ï¸ ã‚¹ã‚­ãƒƒãƒ—ã—ãŸãƒ†ã‚¹ãƒˆ ({len(analysis['skipped_tests'])}å€‹):")
        skip_reasons = {}
        for test in analysis['skipped_tests']:
            reason = "ç†ç”±ä¸æ˜"
            if 'call' in test and 'longrepr' in test['call']:
                longrepr = test['call']['longrepr']
                if isinstance(longrepr, str) and 'SKIPPED' in longrepr:
                    reason = longrepr.split('SKIPPED')[-1].strip().strip('[').strip(']')
            
            if reason not in skip_reasons:
                skip_reasons[reason] = []
            skip_reasons[reason].append(test.get('nodeid', 'Unknown'))
        
        for reason, tests in skip_reasons.items():
            print(f"   ğŸ“‹ {reason}: {len(tests)}å€‹")
            if len(tests) <= 3:
                for test in tests:
                    print(f"      â€¢ {test}")

def print_usage_help():
    """ä½¿ç”¨æ–¹æ³•ã®è¡¨ç¤º"""
    print("\nğŸ’¡ ä¾¿åˆ©ãªpytestã‚³ãƒãƒ³ãƒ‰:")
    print("   pytest -v                    # è©³ç´°è¡¨ç¤ºã§å…¨ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("   pytest --lf                  # å‰å›å¤±æ•—ã—ãŸãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ")
    print("   pytest -k pattern            # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã™ã‚‹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ")
    print("   pytest tests/test_api.py     # ç‰¹å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å®Ÿè¡Œ")
    print("   pytest -m unit               # å˜ä½“ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ")
    print("   pytest -m 'not gui'          # GUIãƒ†ã‚¹ãƒˆä»¥å¤–ã‚’å®Ÿè¡Œ")
    print("   pytest --maxfail=1           # æœ€åˆã®å¤±æ•—ã§åœæ­¢")
    print("   pytest --tb=long             # è©³ç´°ãªãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯è¡¨ç¤º")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸ§ª Pythonåˆå­¦è€…å‘ã‘ãƒ­ãƒ¼ã‚°ãƒ©ã‚¤ã‚¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯")
    print("ğŸ“‹ pytestçµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ")
    
    # pytest availability check
    if not install_pytest_if_needed():
        print("âŒ pytestãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ‰‹å‹•ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install -r requirements.txt")
        return False
    
    # pytest-json-report ã®ç¢ºèªï¼ˆrequirements.txtã«å«ã¾ã‚Œã¦ã„ã‚‹ã¯ãšï¼‰
    try:
        import pytest_jsonreport
    except ImportError:
        print("âš ï¸ pytest-json-reportãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        print("ğŸ’¡ ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
        print("   pip install -r requirements.txt")
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®å‡¦ç†
    pytest_args = sys.argv[1:] if len(sys.argv) > 1 else []
    
    if not pytest_args:
        print("ğŸ” å…¨ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™...")
        pytest_args = ["tests/"]
    
    # pytestå®Ÿè¡Œ
    result = run_pytest_with_json_report(pytest_args)
    
    # çµæœåˆ†æã¨è¡¨ç¤º
    analysis = analyze_test_results(result.get('report', {}))
    print_detailed_results(analysis, result['execution_time'])
    
    # å“è³ªåˆ¤å®š
    if analysis:
        success_rate = (analysis['passed'] / analysis['total']) * 100 if analysis['total'] > 0 else 0
        
        print("\n" + "=" * 80)
        if success_rate >= 90:
            print("ğŸ‰ å“è³ªè©•ä¾¡: å„ªç§€")
            print("âœ¨ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯æœ¬æ ¼é‹ç”¨å¯èƒ½ãªçŠ¶æ…‹ã§ã™")
        elif success_rate >= 70:
            print("â­ å“è³ªè©•ä¾¡: è‰¯å¥½")
            print("ğŸ”§ ä¸€éƒ¨æ”¹å–„ã«ã‚ˆã‚Šå®Œå…¨ãªå‹•ä½œãŒæœŸå¾…ã§ãã¾ã™")
        else:
            print("âš ï¸ å“è³ªè©•ä¾¡: è¦æ”¹å–„")
            print("ğŸ”¨ å¤šãã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ä¿®æ­£ãŒå¿…è¦ã§ã™")
    
    print_usage_help()
    
    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    if os.path.exists("pytest_report.json"):
        os.remove("pytest_report.json")
    
    return result['returncode'] == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)